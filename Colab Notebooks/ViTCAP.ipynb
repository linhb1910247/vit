{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19053,"status":"ok","timestamp":1695107111129,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"SyBzx9XECyQv","outputId":"cf48b8ab-34f9-4bf5-aa44-4c9dce524584"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrF1nAqNONQf"},"outputs":[],"source":["!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n","!unzip Flickr8k_Dataset.zip\n","!rm -rf *.zip\n","!rm -rf __MACOSX"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"o8xkZAykDtZy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695102814410,"user_tz":-420,"elapsed":18822,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"4a35f2ab-7482-44a0-add4-5ad16dff2480"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ViTCAP\n","⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n","📦 Installing...\n","📌 Adjusting configuration...\n","🩹 Patching environment...\n","⏲ Done in 0:00:13\n","🔁 Restarting kernel...\n"]}],"source":["%cd '/content/drive/MyDrive/ViTCAP'\n","!pip install -q condacolab\n","import condacolab\n","condacolab.install()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-nku---WyQ2"},"outputs":[],"source":["with open('/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.caption.tsv','r') as file:\n","  test_data = json.load(file)"]},{"cell_type":"code","source":["with open('/content/test_caption.json','r') as file:\n","  test_data = json.load(file)"],"metadata":{"id":"Fi8wNcoatwTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gD8JSAtaP4jQ"},"outputs":[],"source":["test1 = test_data[0]\n","print(test1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbC11PxhQgYH"},"outputs":[],"source":["root_path = \"/content/Flicker8k_Dataset/\"\n","extension = '.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LwSZd-aQ9wf"},"outputs":[],"source":["test_tsv_data = open(\"test_tsv_data.tsv\", \"a\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_t3rx6gtQm1c"},"outputs":[],"source":["for data in test_data:\n","  with open(root_path+data['image_id']+extension, 'rb') as image_file:\n","      image_data = image_file.read()\n","\n","  base64_encoded = base64.b64encode(image_data).decode('utf-8')\n","\n","  test_tsv_data.write(data['image_id'])\n","  test_tsv_data.write('\\t')\n","  test_tsv_data.write(base64_encoded)\n","test_tsv_data.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VueRsduSDz-D"},"outputs":[],"source":["%cd '/content/drive/MyDrive/ViTCAP'\n","\n","!conda create -n vitcap python=3.8\n","!conda activate vitcap"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yIJsb9rCdQkg","executionInfo":{"status":"ok","timestamp":1695102863844,"user_tz":-420,"elapsed":469,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}}},"outputs":[],"source":["!source activate vitcap"]},{"cell_type":"markdown","metadata":{"id":"OvPTIesbP-LH"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHqsXso1Grer"},"outputs":[],"source":["%cd '/content/drive/MyDrive/ViTCAP'\n","\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SyIVmnZIq22"},"outputs":[],"source":["!pip install timm\n","!pip install transformers\n","\n","!pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zioP3WVPK7zh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28878,"status":"ok","timestamp":1695107235011,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"GLBjuaLaGus7","outputId":"e67f8fb1-45ff-40f7-b22e-3e78ceb64b59"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ViTCAP\n"]}],"source":["%cd '/content/drive/MyDrive/ViTCAP'\n","\n","import os\n","import PIL\n","import math\n","import yaml\n","import torch\n","import logging\n","import requests\n","import numpy as np\n","import torch.nn as nn\n","from PIL import Image\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","from IPython.display import Image\n","import torchvision.transforms as transforms\n","from src.pipelines.uni_pipeline import get_transform_vit_default\n","%matplotlib inline\n","\n","\n","caption_pipeline = './images/caption_pipeline.png'\n","vit_pipeline = './images/ViTCAP.png'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQOOkYl3LNlp"},"outputs":[],"source":["Image(filename=caption_pipeline)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUZwmn9wLQeA"},"outputs":[],"source":["Image(filename=vit_pipeline)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"VudAqm9wLWNc","executionInfo":{"status":"ok","timestamp":1695107235014,"user_tz":-420,"elapsed":6,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}}},"outputs":[],"source":["\n","# Image Captioning model wrapper\n","class ImageCaptioning(nn.Module):\n","    def __init__(self,\n","                 model,\n","                 test_extra_input=None,\n","                 tokenizer=None,\n","                 bert_tokenizer=None,\n","                 image_encoder=None,\n","                 cfg=None,\n","                 ):\n","        super().__init__()\n","        self.module = model\n","        self.iter = 0\n","        self.tokenizer = tokenizer\n","        self.bert_tokenizer = bert_tokenizer\n","        self.test_extra_input = test_extra_input\n","        self.image_encoder = image_encoder\n","        self.cfg = cfg\n","#         self.acc = MultiLabelAccuracy()\n","#         self.map = mAPMeter()\n","\n","        if cfg.pert_img_prob is not None and cfg.pert_img_prob > 0:\n","            # we need an image text matching loss on the pooled output\n","            # number of relationship is always 1, we use BCE loss\n","            self.seq_relationship = nn.Linear(model.bert.pooler.dense.weight.shape[0], 1)\n","            assert self.cfg.mask_type != 'seq2seq', 'matching loss is useless'\n","        else:\n","            self.seq_relationship = None\n","\n","        self.category = cfg.category\n","        if self.category == 'vinvl':\n","            self.vocab = self.tokenizer['idx_to_label']\n","        else:\n","            self.vocab = self.bert_tokenizer.ids_to_tokens\n","\n","    def construct_attn_mask(self, data):\n","        img_feats = data['img_feats']\n","        input_ids = data['input_ids']\n","        attention_mask = data['attention_mask']\n","        batch_size = img_feats.shape[0]\n","\n","        num_img_feats = img_feats.shape[1]\n","        num_token = input_ids.shape[-1]\n","        device = input_ids.device\n","        top_right = torch.ones((batch_size, num_token, num_img_feats), device=device)\n","        if self.cfg.mask_type == 'seqbid':\n","            mask_type = data.pop('mask_type')\n","            bottom_left = torch.ones((batch_size, num_img_feats, num_token), device=device)\n","            # if mask_type is 1, it is seq2seq and we need to zero it out\n","            bottom_left[mask_type] = 0\n","        elif self.cfg.mask_type in ['seq2seq', 'seq2seq_off']:\n","            bottom_left = torch.zeros((batch_size, num_img_feats, num_token), device=device)\n","        else:\n","            assert self.cfg.mask_type == 'bidirectional'\n","            bottom_left = torch.ones((batch_size, num_img_feats, num_token), device=device)\n","            if attention_mask.dim() == 2:\n","                attention_mask = attention_mask.unsqueeze(dim=1)\n","                attention_mask = attention_mask.expand(batch_size, num_token, num_token)\n","        bottom_right = torch.ones((batch_size, num_img_feats, num_img_feats), device=device)\n","        bottom = torch.cat((bottom_left, bottom_right), dim=2)\n","\n","        top = torch.cat((attention_mask, top_right), dim=2)\n","        full_attention_mask = torch.cat((top, bottom), dim=1)\n","        data['attention_mask'] = full_attention_mask\n","\n","    def forward(self, data):\n","\n","        data = dict(data.items())\n","\n","        # this is required in test, but not in train\n","        data.pop('key')\n","        data['gen_tag_ratio'] = 1\n","\n","        assert self.image_encoder\n","        assert 'img_feats' not in data\n","        data['img_feats'] = self.image_encoder(data)\n","\n","        data.pop('image')\n","        self.construct_attn_mask(data)\n","\n","        # Note: this wrapper only contains inference code, refer XX_captioning_uni_pipelineXX.py for full code\n","        # if self.cfg.use_cbs:\n","        #     data.update({\n","        #         'min_constraints_to_satisfy': 2,\n","        #         'use_cbs': True,\n","        #     })\n","\n","        data.update(self.test_extra_input)\n","        result = self.module(**data)\n","\n","        return result"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"68aVrhGtLero","executionInfo":{"status":"ok","timestamp":1695107242486,"user_tz":-420,"elapsed":304,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}}},"outputs":[],"source":["# parameter loading\n","with open(r'/content/drive/MyDrive/ViTCAP/yaml/ViTCAP_Captioning_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1.yaml') as file:\n","    cfg = yaml.full_load(file)\n","\n","\n","class Struct:\n","    def __init__(self, **entries):\n","        self.__dict__.update(entries)\n","\n","cfg['param'].update(cfg['all_test_data'])\n","\n","# update some value\n","cfg['param'].update({\n","    'mask_type': 'seq2seq',\n","    'max_seq_a_length': 40,\n","    'max_seq_length': 70,\n","    'add_od_labels': True,\n","    'od_label_conf ': 0.2,\n","    'drop_out': 0.1,\n","    'tie_weights': True,\n","    'label_smoothing': 0.1,\n","    'img_layer_norm_eps': 1e-5,\n","    'max_img_seq_length': 50,\n","    'max_gen_length': 20,\n","    'output_isvalid': False,\n","    'max_masked_tokens': 3,\n","    'cider_cached_tokens': 'data/coco_caption/gt/coco-train-words.p',\n","    'num_beams': 1,\n","    'mask_prob': 0.15,\n","    'replace_by_mask_prob': 0.8,\n","    'replace_by_rand_prob': 0.1,\n","    'temperature': 1,\n","    'top_k': 0,\n","    'top_p': 1,\n","    'gradient_clip': 1.,\n","    'optimizer_type': 'MAdamW',\n","    'bias_no_weight_decay': True,\n","    'ln_no_weight_decay': True,\n","    'unique_labels_on': False,\n","    'scheduler_type': 'linear',\n","    'pad_to_max': True,\n","    'no_sort_by_conf': False,\n","    'ignore_project_image': False,\n","    'real_text_a_in_test': False,\n","    'pert_img_prob': None,\n","})\n","\n","cfg = Struct(**cfg['param'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kx_c2rR0YX1"},"outputs":[],"source":["%cd /content/drive/MyDrive/ViTCAP\n","!python run.py -c ./yaml/ViTCAP_Captioning_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1.yaml\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EggYUftMUZf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f14ee424-bd87-4630-c2d7-f55eef23c002"},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 5, in <module>\n","    from pip._internal.cli.main import main\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n","    from pip._internal.cli.autocompletion import autocomplete\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n","    from pip._internal.cli.main_parser import create_main_parser\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n","    from pip._internal.build_env import get_runnable_pip\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n","    from pip._internal.cli.spinners import open_spinner\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n","    from pip._internal.utils.logging import get_indentation\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n","    from pip._internal.utils.misc import ensure_dir\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 43, in <module>\n","    from pip._internal.exceptions import CommandError, ExternallyManagedEnvironment\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/exceptions.py\", line 18, in <module>\n","    from pip._vendor.requests.models import Request, Response\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/requests/__init__.py\", line 43, in <module>\n","    from pip._vendor import urllib3\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/__init__.py\", line 15, in <module>\n","    from .poolmanager import PoolManager, ProxyManager, proxy_from_url\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/poolmanager.py\", line 76, in <module>\n","    ProxyConfig = collections.namedtuple(\"ProxyConfig\", _proxy_config_fields)\n","  File \"/usr/lib/python3.10/collections/__init__.py\", line 481, in namedtuple\n","    result = type(typename, (tuple,), class_namespace)\n","KeyboardInterrupt\n","^C\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: progressbar in /usr/local/lib/python3.10/dist-packages (2.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Collecting boto3\n","  Using cached boto3-1.28.50-py3-none-any.whl (135 kB)\n","Collecting botocore<1.32.0,>=1.31.50 (from boto3)\n","  Using cached botocore-1.31.50-py3-none-any.whl (11.2 MB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n","  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n","  Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.50->boto3) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4 (from botocore<1.32.0,>=1.31.50->boto3)\n","  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.50->boto3) (1.16.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.4\n","    Uninstalling urllib3-2.0.4:\n","      Successfully uninstalled urllib3-2.0.4\n","Successfully installed boto3-1.28.50 botocore-1.31.50 jmespath-1.0.1 s3transfer-0.6.2 urllib3-1.26.16\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"]}],"source":["!pip install future\n","!pip install matplotlib\n","!pip install progressbar\n","!pip install nltk\n","!pip install boto3\n","!pip install opencv-python\n"]},{"cell_type":"code","source":["!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"],"metadata":{"id":"0ZTpRST0C5v8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRJMJTFgOKUT"},"outputs":[],"source":["#Load ViTCAP model\n","%cd '/content/drive/MyDrive/ViTCAP'\n","from src.layers.bert import BertTokenizer,BertConfig\n","\n","# property\n","def tag_vocab():\n","    assert op.isfile(cfg.tokenizer_file)\n","    tag_vocab = json.load(open(cfg.tokenizer_file))\n","    return tag_vocab\n","\n","\n","# property\n","def get_tokenizer(cfg):\n","    tokenizer = BertTokenizer.from_pretrained(\n","       cfg.text_encoder_type, do_lower_case=True)\n","    return tokenizer\n","\n","\n","config = BertConfig.from_pretrained(\n","    # this class does not support a separate text encoder and thus\n","    # text_encoder_type here means the joint fusion encoder\n","    cfg.text_encoder_type,\n","    num_labels=2,\n","    finetuning_task='image_captioning',\n",")\n","\n","\n","if 'vit' in cfg.text_encoder_type:\n","    # this is just to make sure we are using the right variables for vit model\n","    assert cfg.drop_out == 0\n","\n","config.img_feature_type = 'frcnn'\n","config.hidden_dropout_prob = cfg.drop_out\n","config.loss_type = 'classification'\n","config.tie_weights = False\n","config.freeze_embedding = False\n","config.label_smoothing = False\n","config.drop_worst_ratio = 0\n","config.drop_worst_after = 0\n","config.img_feature_dim = cfg.img_feature_dim\n","config.use_img_layernorm = cfg.use_img_layernorm\n","config.img_layer_norm_eps = 0\n","config.net = cfg.image_encoder_type[len('VitEmb_'):]\n","config.ignore_project_image = cfg.ignore_project_image\n","config.later_captioning = 7\n","config.attn_token_sample = 0.1\n","config.vocab = tag_vocab\n","config.tokenizer = get_tokenizer(cfg)\n","config.loss = getattr(cfg, 'loss', 'bce')\n","config.split_blocks = getattr(cfg, 'split_blocks', '4')\n","config.topktagger = getattr(cfg, 'topktagger', None)\n","config.topk = 50\n","# config.tagemb = getattr(cfg, 'tagemb', None) # cls/bert\n","config.tagemb = None\n","config.tagemb_gradient = getattr(cfg, 'tagemb_gradient', None)\n","config.category = getattr(cfg, 'category', 'vinvl')\n","config.tie_tag_weights = getattr(cfg, 'tie_tag_weights', False)\n","\n","\n","from src.layers.bert import ViTCAP\n","# TaggerEncDecCLSEmbSplitBertImgModel\n","\n","# there might have logging warning error, ignore them\n","model = ViTCAP(config=config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcTJsmgVLol8"},"outputs":[],"source":["%cd '/content/drive/MyDrive/ViTCAP'\n","\n","!python run.py -c ./yaml/ViTCAP_Captioning_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1.yaml\n"]},{"cell_type":"code","source":["!kill 9\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhNC8--6Ps_b","executionInfo":{"status":"ok","timestamp":1695107070928,"user_tz":-420,"elapsed":303,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"c389babe-b76f-4504-9dd2-793211ba7c68"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: kill: (9) - No such process\n"]}]},{"cell_type":"code","source":["/content/drive/MyDrive/ViTCAP/src/data_layer/dataset.py"],"metadata":{"id":"RixLAZaEddS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.hw.tsv') as f:\n","    for line in f:\n","        key, str_hw = line.strip().split('\\t')\n","        print(key, str_hw)"],"metadata":{"id":"LW7YZIa4dN3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XY7eIRctOhlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load an image and apply data augmentation.\n","\n","# TIMM style data pre-processing\n","class BGR2RGB(object):\n","    def __call__(self, im):\n","        return im[:, :, [2, 1, 0]]\n","\n","\n","def get_transform_vit_default(cfg):\n","    normalize = transforms.Normalize(\n","        mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","    trans = [\n","        BGR2RGB(),\n","        transforms.ToPILImage(),\n","    ]\n","\n","    trans.extend([\n","        transforms.Resize(int(math.floor(cfg.test_crop_size / cfg.crop_pct)), PIL.Image.BICUBIC),\n","        transforms.CenterCrop(cfg.test_crop_size),\n","    ]),\n","    trans.extend([\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","    transform = transforms.Compose(trans)\n","\n","    return transform\n","\n","\n","transform = get_transform_vit_default(cfg)"],"metadata":{"id":"zg_m3qUdOiev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import json\n","\n","def fix_json_in_tsv(input_file, output_file):\n","    with open(input_file, 'r') as tsv_file, open(output_file, 'w', newline='') as fixed_file:\n","        tsv_reader = csv.DictReader(tsv_file, delimiter='\\t')\n","        fieldnames = tsv_reader.fieldnames\n","        tsv_writer = csv.DictWriter(fixed_file, fieldnames=fieldnames, delimiter='\\t')\n","        tsv_writer.writeheader()\n","\n","        for row in tsv_reader:\n","            # Xác định cột chứa dữ liệu JSON không hợp lệ (ví dụ: 'json_data')\n","            json_column = 'json_data'\n","            if json_column in row:\n","                json_data = row[json_column]\n","                try:\n","                    # Cố gắng sửa lỗi cú pháp JSON\n","                    fixed_json = json.loads(json_data)\n","                    row[json_column] = json.dumps(fixed_json)\n","                except json.JSONDecodeError as e:\n","                    print(f\"Lỗi cú pháp JSON trong dòng {row}: {e}\")\n","            tsv_writer.writerow(row)\n","\n","# Thực hiện sửa lỗi trong tệp TSV\n","input_file = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.hw.tsv'\n","output_file = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/fixed.tsv'\n","fix_json_in_tsv(input_file, output_file)\n","\n"],"metadata":{"id":"zszpdHdrLTfq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0ltyvKg4L3pY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["/content/drive/MyDrive/ViTCAP/src/data_layer/transform.py"],"metadata":{"id":"axQ7VOIbbLMz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","json_str = '{\"key\": \"value\", \"nested\": {\"inner_key\": \"inner_value\"}}'\n","try:\n","    json_obj = json.loads(json_str)\n","    # Kiểm tra cấu trúc JSON\n","    if isinstance(json_obj, dict):\n","        print(\"JSON object is a dictionary.\")\n","    elif isinstance(json_obj, list):\n","        print(\"JSON object is a list.\")\n","    # Nếu bạn muốn kiểm tra cụ thể cấu trúc, bạn có thể truy cập các phần tử bằng key hoặc index.\n","    print(json_obj[\"key\"])\n","except json.JSONDecodeError as e:\n","    print(\"Invalid JSON:\", e)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEpY4UKYafrz","executionInfo":{"status":"ok","timestamp":1694925217665,"user_tz":-420,"elapsed":333,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"bda1e342-5c4c-4add-c2ce-eb0702973f79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["JSON object is a dictionary.\n","value\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"2wpZJOkgW3Of"}},{"cell_type":"code","source":["import json\n","\n","# Đường dẫn đến tệp JSON đầu vào và TSV đầu ra\n","json_file_path = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/output_json_file.json'\n","tsv_file_path = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.hw.tsv'\n","\n","# Đọc dữ liệu từ tệp JSON\n","with open(json_file_path, 'r') as json_file:\n","    data = json.load(json_file)\n","\n","# Mở tệp TSV đầu ra để viết\n","with open(tsv_file_path, 'w') as tsv_file:\n","    # Viết tiêu đề cho tệp TSV\n","\n","    # Viết dữ liệu từ JSON vào TSV với cấu trúc đúng\n","    for key, value in data.items():\n","        json_value = json.dumps(value)\n","        tsv_file.write(f\"{key}\\t{json_value}\\n\")\n","\n","print(f\"Dữ liệu đã được lưu vào tệp TSV: {tsv_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fojWlJZbXN8T","executionInfo":{"status":"ok","timestamp":1694794419847,"user_tz":-420,"elapsed":284,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"72c27b5d-9700-4512-bcac-d388ac9fcf8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dữ liệu đã được lưu vào tệp TSV: /content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.hw.tsv\n"]}]},{"cell_type":"code","source":["import json\n","\n","# Đường dẫn đến tệp TSV đầu vào và JSON đầu ra\n","tsv_file_path = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/train.hw.tsv'\n","json_file_path = '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/output_json_file.json'\n","\n","# Đọc dữ liệu từ tệp TSV\n","data = {}\n","with open(tsv_file_path, 'r') as tsv_file:\n","    for line in tsv_file:\n","        parts = line.strip().split('\\t')\n","        if len(parts) == 2:\n","            key, json_str = parts\n","            try:\n","                json_data = json.loads(json_str)\n","                data[key] = json_data\n","            except json.JSONDecodeError:\n","                print(f\"Không thể phân tích dòng: {line}\")\n","\n","# Lưu dữ liệu dưới dạng JSON vào tệp mới\n","with open(json_file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=4)\n","\n","print(f\"Dữ liệu đã được lưu vào tệp JSON: {json_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6_63Tmtmj6-","executionInfo":{"status":"ok","timestamp":1694794374076,"user_tz":-420,"elapsed":340,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"92f092cd-bc47-4d69-e960-5ac568f4b2b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dữ liệu đã được lưu vào tệp JSON: /content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/output_json_file.json\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"-HNG6K_RFAeE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOo4vsDb9EEC"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyceRPass6ZU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695021135699,"user_tz":-420,"elapsed":20181,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"ef878bb7-c8c6-409c-dd5d-b9b665cfed41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Collecting regex!=2019.12.17\n","  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.3.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.12.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers) (23.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/site-packages (from transformers) (0.17.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.8.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, regex, transformers\n","Successfully installed regex-2023.8.8 tokenizers-0.13.3 transformers-4.33.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6XsjDlo2wJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694700644318,"user_tz":-420,"elapsed":3581,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"bd88c143-7f22-472d-cc3a-74c0d23f68e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["helpers.py  <All keys matched successfully>\n"]}],"source":[" #wrap the model into a captioning pipeline\n","# seperate out the linear projection as image encoder, this makes the training/inference easier\n","def get_image_encoder_model(is_train=False):\n","    if cfg.image_encoder_type.startswith('VitEmb_'):\n","        # VitEmb_base32_384\n","        net = cfg.image_encoder_type[len('VitEmb_'):]\n","\n","        if cfg.image_encoder_pretrained:\n","            logging.info('VIT image encoder loaded from pre-trained weight!  '\n","                         'Note that this might be replaced by pre-trained checkpoint later!')\n","        from src.pytorch_image_models import timm\n","\n","        logging.info('Non-Patch Selection Mode.')\n","        model = timm.create_model(\n","            net,\n","            output_grid=True,\n","            pretrained=cfg.image_encoder_pretrained,\n","        )\n","\n","        # clear out the following two modules\n","        model.norm = nn.Identity()\n","        model.blocks = nn.ModuleList()\n","        if not is_train:\n","            model.eval()\n","\n","        from src.tools.torch_common import InputAsDict\n","        model = InputAsDict(model)\n","\n","    else:\n","        raise NotImplementedError(self.cfg.image_encoder_type)\n","    return model\n","\n","# image encoder: one linear projection\n","image_encoder = get_image_encoder_model(is_train=False)\n","\n","# testing mode\n","tokenizer = get_tokenizer(cfg)\n","cls_token_id, sep_token_id, pad_token_id, mask_token_id, period_token_id = \\\n","    tokenizer.convert_tokens_to_ids([\n","        tokenizer.cls_token,\n","        tokenizer.sep_token,\n","        tokenizer.pad_token,\n","        tokenizer.mask_token,\n","        '.',\n","    ])\n","\n","test_extra_input = {\n","    'is_decode': True,\n","    'do_sample': False,\n","    'bos_token_id': cls_token_id,\n","    'pad_token_id': pad_token_id,\n","    'eos_token_ids': [sep_token_id],\n","    'mask_token_id': mask_token_id,\n","    # for adding od labels\n","    'add_od_labels': cfg.add_od_labels,\n","    'od_labels_start_posid': cfg.max_seq_a_length,\n","    # hyper-parameters of beam search\n","    'max_length': cfg.max_gen_length,\n","    'num_beams': cfg.num_beams,\n","    \"temperature\": cfg.temperature,\n","    \"top_k\": cfg.top_k,\n","    \"top_p\": cfg.top_p,\n","    \"repetition_penalty\": 1,\n","    \"length_penalty\": 1,\n","    \"num_return_sequences\": 1,\n","    \"num_keep_best\": 1,\n","}\n","\n","\n","model = ImageCaptioning(\n","    model,\n","    test_extra_input,\n","    tokenizer=tokenizer,\n","    bert_tokenizer=tokenizer,\n","    image_encoder=image_encoder,\n","    cfg=cfg,\n",")"]},{"cell_type":"code","source":["# Load an image and apply data augmentation.\n","\n","# TIMM style data pre-processing\n","class BGR2RGB(object):\n","    def __call__(self, im):\n","        return im[:, :, [2, 1, 0]]\n","\n","\n","def get_transform_vit_default(cfg):\n","    normalize = transforms.Normalize(\n","        mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","\n","    trans = [\n","        BGR2RGB(),\n","        transforms.ToPILImage(),\n","    ]\n","\n","    trans.extend([\n","        transforms.Resize(int(math.floor(cfg.test_crop_size / cfg.crop_pct)), PIL.Image.BICUBIC),\n","        transforms.CenterCrop(cfg.test_crop_size),\n","    ]),\n","    trans.extend([\n","        transforms.ToTensor(),\n","        normalize,\n","    ])\n","    transform = transforms.Compose(trans)\n","\n","    return transform\n","\n","\n","transform = get_transform_vit_default(cfg)"],"metadata":{"id":"dgDqwcBwCf5M"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7gBentcDyY8","colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"status":"error","timestamp":1694700757706,"user_tz":-420,"elapsed":7,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"}},"outputId":"be435be2-128a-4634-d736-84d3cb4b89a7"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7aaef2a72d98>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign_and_update_state_dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqd_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_state_ignore_mismatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/checkpoint/Logit_Vilt_captioning_testing_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1_expand_tag-classifier_emb.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/checkpoint/Logit_Vilt_captioning_testing_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1_expand_tag-classifier_emb.zip'"]}],"source":["# load from the checkpoint\n","from src.tools.opt.checkpoint import align_and_update_state_dicts\n","from src.tools.qd_pytorch import load_model_state_ignore_mismatch\n","checkpoint = torch.load(\"/content/drive/MyDrive/ViTCAP/data/ViTCAPCOCO/checkpoint/Logit_Vilt_captioning_testing_batch-size_512_encoder_vit_base_patch16_384_lr_1e-4_iter_60_vitbfocal20_bert_tokenizer_tags_ENC-DEC_multiplier_0.1_expand_tag-classifier_emb.zip\")\n","\n","\n","def strip_prefix_if_present(state_dict, prefix):\n","    from src.tools.torch_common import remove_prefix\n","    return remove_prefix(state_dict, prefix)\n","\n","model_state_dict = model.state_dict()\n","\n","# if the state_dict comes from a model that was wrapped in a\n","# DataParallel or DistributedDataParallel during serialization,\n","# remove the \"module\" prefix before performing the matching\n","loaded_state_dict = strip_prefix_if_present(checkpoint['model'], prefix=\"module.\")\n","align_and_update_state_dicts(model_state_dict, loaded_state_dict)\n","\n","# use strict loading\n","# model.load_state_dict(model_state_dict)\n","\n","# use non-strict loading\n","load_model_state_ignore_mismatch(model, model_state_dict)\n","\n","model.cuda()\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0LmJS_3DpCA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7222,"status":"ok","timestamp":1693460461986,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"RhtoijWR4wyJ","outputId":"89234942-fc4d-45f3-fe98-9338a64b5b2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.1+cu118\n"]}],"source":["import torch\n","print(torch.__version__)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1693922574105,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"b0yMrLNIYs1W","outputId":"f5c08442-5508-4d1a-deaf-8e2f103e78d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv/Images\n"]}],"source":["import pandas as pd\n","import json\n","import numpy as np\n","import regex as re\n","\n","\n","%cd '/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv/Images'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":549,"status":"ok","timestamp":1693922576758,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"9McWLaNWdAfs","outputId":"a1fc50cc-a929-49da-b824-9c35155c63ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv\n","40455\n"]},{"data":{"text/html":["\n","  <div id=\"df-6a3e7065-8950-4ea1-a084-953901bf6e29\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>caption</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000268201_693b08cb0e.jpg</td>\n","      <td>A child in a pink dress is climbing up a set o...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000268201_693b08cb0e.jpg</td>\n","      <td>A girl going into a wooden building .</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000268201_693b08cb0e.jpg</td>\n","      <td>A little girl climbing into a wooden playhouse .</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000268201_693b08cb0e.jpg</td>\n","      <td>A little girl climbing the stairs to her playh...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000268201_693b08cb0e.jpg</td>\n","      <td>A little girl in a pink dress going into a woo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a3e7065-8950-4ea1-a084-953901bf6e29')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6a3e7065-8950-4ea1-a084-953901bf6e29 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6a3e7065-8950-4ea1-a084-953901bf6e29');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-586e1eee-7883-4fcc-8098-c745489bfc33\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-586e1eee-7883-4fcc-8098-c745489bfc33')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-586e1eee-7883-4fcc-8098-c745489bfc33 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                       image  \\\n","0  1000268201_693b08cb0e.jpg   \n","1  1000268201_693b08cb0e.jpg   \n","2  1000268201_693b08cb0e.jpg   \n","3  1000268201_693b08cb0e.jpg   \n","4  1000268201_693b08cb0e.jpg   \n","\n","                                             caption  \n","0  A child in a pink dress is climbing up a set o...  \n","1              A girl going into a wooden building .  \n","2   A little girl climbing into a wooden playhouse .  \n","3  A little girl climbing the stairs to her playh...  \n","4  A little girl in a pink dress going into a woo...  "]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["%cd '/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv'\n","df = pd.read_csv('captions.txt', sep=',')\n","print(df.shape[0])\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVCC_CBYdWZ-"},"outputs":[],"source":["split = 'test'\n","\n","# Remove file extension '.jpg'\n","df['image'] = df['image'].apply(lambda row : row[:-4])\n","\n","# Make clean captions\n","#reg = re.compile('[^a-zA-Z, ]+')\n","#df['caption'] = df['caption'].apply(lambda row : re.sub(reg, '', row).strip()+'.')\n","df['caption'] = df['caption'].apply(lambda row : row.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693922581081,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"3rMPz2CRdZpA","outputId":"e37c5b46-d9a0-45cb-ef16-948ee17d5a47"},"outputs":[{"data":{"text/plain":["0    1000268201_693b08cb0e\n","1    1000268201_693b08cb0e\n","2    1000268201_693b08cb0e\n","3    1000268201_693b08cb0e\n","4    1000268201_693b08cb0e\n","Name: image, dtype: object"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["df['image'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1693922582715,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"ebXaEGGBdbQP","outputId":"bd14458e-2141-4ae9-ebd9-2dc6a7700008"},"outputs":[{"data":{"text/plain":["0    A child in a pink dress is climbing up a set o...\n","1                A girl going into a wooden building .\n","2     A little girl climbing into a wooden playhouse .\n","3    A little girl climbing the stairs to her playh...\n","4    A little girl in a pink dress going into a woo...\n","Name: caption, dtype: object"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["df['caption'].head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56qSDffCdf5Q"},"outputs":[],"source":["# Create image annotations\n","images = [{\"file_name\" : df['image'].iloc[i],\"id\" : df['image'].iloc[i]} for i in range(df.shape[0])]\n","annotations ={\"images\" : images}\n","# Create coco_format\n","coco_format = {\"info\": \"dummy\",\n","               \"type\": \"captions\",\n","               \"licenses\": \"dummy\",\n","               \"images\" : images}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K78TfbEbEoJY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRL1aVl8dh06"},"outputs":[],"source":["# dump json\n","with open(split+\"_caption.json\", \"w\") as f:\n","  json.dump(annotations, f, separators=(', ', ': '), ensure_ascii = False)\n","\n","with open(split+\"_caption_coco_format.json\", \"w\") as f:\n","  json.dump(coco_format, f, separators=(', ', ': '), ensure_ascii = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1693228067269,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"J8FDKM2ydzTq","outputId":"dc1cd5ca-7538-4b68-fd9c-c70d403386d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ViTCAP/data\n"]}],"source":["%cd /content/drive/MyDrive/ViTCAP/data\n"]},{"cell_type":"markdown","metadata":{"id":"Irx8TOEumqQk"},"source":["oscar demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFZMRBcrn9ma"},"outputs":[],"source":["# install pytorch 1.7\n","! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","\n","# install apex\n","%cd /content/drive/MyDrive/ViTCAP/data/apex\n","! python setup.py install --cuda_ext --cpp_ext\n","\n","# install oscar\n","%cd /content/drive/MyDrive/ViTCAP/data/Oscar\n","! pip install pycocoevalcap\n","! chmod u+r+x get_stanford_models.sh\n","! ./get_stanford_models.sh\n","%cd ..\n","! python setup.py build develop\n","%cd /content/drive/MyDrive/ViTCAP/data/Oscar\n","# install requirements\n","! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WimxZNi4t8Ia"},"outputs":[],"source":["%cd /content/drive/MyDrive/ViTCAP/data/Oscar\n","# install requirements\n","! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1693123266328,"user":{"displayName":"Nguyen Hoang Linh B1910247","userId":"07515145635251418919"},"user_tz":-420},"id":"4bFYLYReArJ8","outputId":"96361768-f303-4a26-fca2-eb928277bfa5"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ViTCAP/data/Oscar\n"]}],"source":["%cd /content/drive/MyDrive/ViTCAP/data/Oscar"]},{"cell_type":"markdown","metadata":{"id":"njl0YRt6oBbQ"},"source":["sgg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpP_GBW4inDi"},"outputs":[],"source":["%cd '/content/drive/MyDrive/ViTCAP/data'\n","import sys\n","import os\n","import os.path as op\n","import yaml\n","import pandas as pd\n","import ast\n","import json\n","import base64\n","import torch\n","import numpy as np\n","np.set_printoptions(suppress=True, precision=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwxXyQF0i4iU"},"outputs":[],"source":["#%cd '/content/drive/MyDrive/ViTCAP/data'\n","\n","# Clone repository và tải các tập tin cần thiết\n","#!git clone https://github.com/microsoft/scene_graph_benchmark.git\n","\n","#!wget https://drive.google.com/file/d/1nvu8y4zZFbJqSqLdQClvMyzsbBeX-oaQ/view\n","\n","#!wget https://drive.google.com/file/d/1M1nPtMPHS1GXx5HvKMMDxOksxQgwX14_/view\n","\n","%cd /content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark\n","\n","  # Cài đặt các phụ thuộc\n","!pip install ninja yacs>=0.1.8 cython matplotlib tqdm opencv-python numpy>=1.19.5\n","\n","!pip install -U PyYAML\n","\n","  # Cài đặt pycocotools\n","!pip install pycocotools\n","\n","  # Cài đặt cityscapesScripts\n","!python -m pip install cityscapesscripts\n","\n","  # Cài đặt thư viện với symbolic links\n","!python setup.py build develop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbHNK5K_nHbS"},"outputs":[],"source":["!conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PyTZUs0ljv1t"},"outputs":[],"source":["%cd '/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark'\n","\n","import os.path\n","import yaml\n","! python /content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv/tsv_demo.py\n","\n","TSV_DIR = '/content/drive/MyDrive/ViTCAP/data/scene_graph_benchmark/tools/mini_tsv/data/'\n","# Create .tsv files for our images\n","\n","# Create .yaml file for connecting .tsv files\n","yaml_dict = {\"img\" : \"train.tsv\",\n","            \"label\" :  \"train.label.tsv\",\n","            \"hw\" : \"train.hw.tsv\",\n","            \"linelist\" : \"train.linelist.tsv\"}\n","with open(os.path.join(TSV_DIR, 'train.yaml'), 'w') as file:\n","        yaml.dump(yaml_dict, file)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP3u/AdERb24v83oDm2+BZ3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}